# CIFAR-10 ResNet18 ëª¨ë¸ ì••ì¶• í”„ë¡œì íŠ¸

**ê³¼ëª©**: GEV6152 Model Compression  
**í”„ë¡œì íŠ¸**: ì¤‘ê°„ í”„ë¡œì íŠ¸ - Pruningì„ í†µí•œ CIFAR-10 ëª¨ë¸ ì••ì¶•

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” CIFAR-10 ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì— ëŒ€í•´ ResNet18 ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ ê°€ì§€ neural network pruning ê¸°ë²•ì„ êµ¬í˜„í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤. ëª©í‘œëŠ” í¬ì†Œì„±(sparsity)-ì •í™•ë„(accuracy) ê°„ì˜ trade-offë¥¼ ë¶„ì„í•˜ê³  ëª¨ë¸ ì••ì¶•ì„ í†µí•œ íš¨ìœ¨ì„± í–¥ìƒì„ ì¸¡ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### êµ¬í˜„ëœ Pruning ë°©ë²•

1. **Magnitude-based Pruning**: ì ˆëŒ€ê°’ì´ ì‘ì€ ê°€ì¤‘ì¹˜ë¥¼ ì œê±°
2. **Structured Pruning**: ì¤‘ìš”ë„ê°€ ë‚®ì€ í•„í„°/ì±„ë„ ë‹¨ìœ„ë¡œ ì œê±°
3. **Lottery Ticket Hypothesis**: Pruning í›„ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ë˜ëŒë ¤ ì¬í•™ìŠµ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.8 ì´ìƒ
- CUDA ì§€ì› GPU (ê¶Œì¥)
- 10GB ì´ìƒì˜ ì—¬ìœ  ë””ìŠ¤í¬ ê³µê°„

### ì„¤ì¹˜ ë°©ë²•

```bash
# í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™
cd project/

# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### ì‹¤í—˜ ì‹¤í–‰

#### ì „ì²´ ì‹¤í—˜ (ëª¨ë“  ë°©ë²•, ëª¨ë“  ì‹œë“œ)
```bash
./run.sh
```

ì‹¤í–‰ ë‚´ìš©:
1. 3ê°œì˜ dense baseline ëª¨ë¸ í•™ìŠµ (ì‹œë“œ: 42, 123, 456)
2. 3ê°€ì§€ pruning ë°©ë²•ì„ 5ê°œ í¬ì†Œì„± ìˆ˜ì¤€ì— ì ìš©
3. ëª¨ë“  pruned ëª¨ë¸ fine-tuning
4. ëª¨ë“  ëª¨ë¸ í‰ê°€
5. ê·¸ë˜í”„ ë° í‘œ ìƒì„±

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: GPU 1ê°œ ê¸°ì¤€ 24-48ì‹œê°„

#### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
./run.sh --quick
```

ì—í­ ìˆ˜ë¥¼ ì¤„ì´ê³  ì¼ë¶€ êµ¬ì„±ë§Œ ì‹¤í–‰ (1-2ì‹œê°„ ì†Œìš”)

#### Dense í•™ìŠµ ê±´ë„ˆë›°ê¸°
ì´ë¯¸ í•™ìŠµëœ dense ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°:
```bash
./run.sh --skip-dense
```

### ê°œë³„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

#### Dense Baseline í•™ìŠµ
```bash
python train_dense.py --seed 42 --epochs 200
```

#### Pruned ëª¨ë¸ í•™ìŠµ
```bash
python train_pruned.py \
    --method magnitude \
    --sparsity 0.9 \
    --seed 42 \
    --finetune-epochs 100
```

#### ëª¨ë“  ëª¨ë¸ í‰ê°€
```bash
python evaluate_all.py
```

#### ì‹œê°í™” ìƒì„±
```bash
python plot_results.py
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project/
â”œâ”€â”€ models/                     # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ resnet.py              # ResNet18 êµ¬í˜„
â”œâ”€â”€ pruning/                    # Pruning ë°©ë²•ë“¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ magnitude_pruning.py   # Magnitude-based pruning
â”‚   â”œâ”€â”€ structured_pruning.py  # Structured pruning
â”‚   â””â”€â”€ lottery_ticket.py      # Lottery Ticket Hypothesis
â”œâ”€â”€ utils/                      # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # í•™ìŠµ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ evaluate.py            # í‰ê°€ ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ metrics.py             # ë©”íŠ¸ë¦­ ê³„ì‚°
â”œâ”€â”€ experiments/                # ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ checkpoints/           # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ results/               # ê²°ê³¼, ê·¸ë˜í”„, í‘œ
â”œâ”€â”€ data/                       # CIFAR-10 ë°ì´í„°ì…‹ (ìë™ ë‹¤ìš´ë¡œë“œ)
â”œâ”€â”€ train_dense.py             # Dense baseline í•™ìŠµ
â”œâ”€â”€ train_pruned.py            # Pruned ëª¨ë¸ í•™ìŠµ
â”œâ”€â”€ evaluate_all.py            # ëª¨ë“  ëª¨ë¸ í‰ê°€
â”œâ”€â”€ plot_results.py            # ì‹œê°í™” ìƒì„±
â”œâ”€â”€ run.sh                     # ë©”ì¸ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”œâ”€â”€ REPORT_KR.md               # í•œê¸€ ë³´ê³ ì„œ
â””â”€â”€ README.md                  # ë³¸ íŒŒì¼
```

## ğŸ“Š ê²°ê³¼

ì‹¤í—˜ ì‹¤í–‰ í›„ ê²°ê³¼ëŠ” `experiments/results/`ì— ì €ì¥ë©ë‹ˆë‹¤:

### ì£¼ìš” ì¶œë ¥ë¬¼

1. **Tradeoff Curve** (`tradeoff_curve.png`)
   - ëª¨ë“  ë°©ë²•ì˜ í¬ì†Œì„± vs ì •í™•ë„ ê·¸ë˜í”„
   - 95% ì‹ ë¢°êµ¬ê°„ í¬í•¨

2. **íš¨ìœ¨ì„± ë¹„êµ í‘œ** (`efficiency_table.md`)
   - ëª¨ë¸ í¬ê¸°, íŒŒë¼ë¯¸í„°, ì •í™•ë„ ë¹„êµ
   - Markdown, LaTeX, CSV í˜•ì‹ ì œê³µ

3. **ì›ì‹œ ë°ì´í„°** (`all_results.json`)
   - ì™„ì „í•œ ì‹¤í—˜ ë°ì´í„°
   - ëª¨ë“  êµ¬ì„±ì— ëŒ€í•œ ëª¨ë“  ë©”íŠ¸ë¦­ í¬í•¨

### ê²°ê³¼ ë°ì´í„° êµ¬ì¡° ì˜ˆì‹œ

```json
{
    "method": "magnitude",
    "sparsity": 0.90,
    "seed": 42,
    "test_accuracy": 92.7,
    "params_millions": 1.12,
    "model_size_mb": 6.19,
    "inference_latency_ms": 1.9
}
```

## ğŸ”¬ ì‹¤í—˜ ì„¤ì •

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

#### Dense ëª¨ë¸ í•™ìŠµ
- **ì•„í‚¤í…ì²˜**: ResNet18 (11.17M íŒŒë¼ë¯¸í„°)
- **ì—í­**: 200
- **ë°°ì¹˜ í¬ê¸°**: 128
- **ì˜µí‹°ë§ˆì´ì €**: SGD with momentum (0.9)
- **í•™ìŠµë¥ **: 0.1 (cosine annealing)
- **ê°€ì¤‘ì¹˜ ê°ì‡ **: 5e-4
- **ë°ì´í„° ì¦ê°•**: Random crop, horizontal flip

#### Pruned ëª¨ë¸ Fine-tuning
- **ì—í­**: 100
- **í•™ìŠµë¥ **: 0.01 (cosine annealing)
- **ê¸°íƒ€ íŒŒë¼ë¯¸í„°**: Dense í•™ìŠµê³¼ ë™ì¼

### í…ŒìŠ¤íŠ¸í•œ í¬ì†Œì„± ìˆ˜ì¤€
- 0.0 (dense baseline)
- 0.3 (30% sparse)
- 0.5 (50% sparse)
- 0.7 (70% sparse)
- 0.9 (90% sparse)
- 0.95 (95% sparse)

### Random Seeds
- 42, 123, 456 (í†µê³„ì  ìœ ì˜ì„± í™•ë³´)

## ğŸ“ˆ ì£¼ìš” ë°œê²¬

### ì‹¤í—˜ ê²°ê³¼

1. **Magnitude Pruning**
   - 90% í¬ì†Œì„±ê¹Œì§€ ìš°ìˆ˜í•œ ì •í™•ë„ ìœ ì§€ (92.7%)
   - êµ¬í˜„ ë³µì¡ë„ ë‚®ìŒ
   - íŠ¹ìˆ˜ í•˜ë“œì›¨ì–´ ì—†ì´ëŠ” ì‹¤ì œ ì†ë„ í–¥ìƒ ì—†ìŒ

2. **Structured Pruning**
   - ë†’ì€ í¬ì†Œì„±ì—ì„œ ì •í™•ë„ ë‹¤ì†Œ í•˜ë½ (90.3%)
   - í‘œì¤€ í•˜ë“œì›¨ì–´ì—ì„œ ì‹¤ì œ ì†ë„ í–¥ìƒ (2.1ë°°)
   - ëª¨ë¸ í¬ê¸° ê°ì†Œ

3. **Lottery Ticket**
   - Magnitude pruningê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ (92.5%)
   - ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œë¶€í„° ì¬í•™ìŠµ í•„ìš”
   - ìµœì  ë¶€ë¶„ ë„¤íŠ¸ì›Œí¬ ì°¾ê¸°ì— íš¨ê³¼ì 

### ì¸¡ì •ëœ ë©”íŠ¸ë¦­

- **Test Accuracy**: CIFAR-10 í…ŒìŠ¤íŠ¸ì…‹ì—ì„œì˜ Top-1 ì •í™•ë„
- **Sparsity**: 0ì¸ ê°€ì¤‘ì¹˜ì˜ ë¹„ìœ¨
- **Parameters**: 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜
- **Model Size**: ë””ìŠ¤í¬ ì €ì¥ í¬ê¸° (MB)
- **Latency**: ì´ë¯¸ì§€ë‹¹ ì¶”ë¡  ì‹œê°„ (ms)

## ğŸ› ï¸ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### Pruning ì•Œê³ ë¦¬ì¦˜

#### Magnitude Pruning
```python
# ì „ì—­ magnitude-based pruning
pruned_model = magnitude_prune_global(model, sparsity=0.9)
```

#### Structured Pruning
```python
# í•„í„° ë‹¨ìœ„ structured pruning
pruned_model = structured_prune_filters(model, sparsity=0.9)
```

#### Lottery Ticket
```python
# Prune í›„ ì´ˆê¸°ê°’ìœ¼ë¡œ ë¦¬ì…‹
pruned_model = lottery_ticket_prune(model, initial_weights, sparsity=0.9)
```

### ë©”íŠ¸ë¦­ ê³„ì‚°

ëª¨ë“  ë©”íŠ¸ë¦­ì€ `utils/metrics.py`ì˜ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤:

```python
from utils import get_model_info

info = get_model_info(model, device='cuda', verbose=True)
# ë°˜í™˜ê°’: params, sparsity, size, latency
```

##  ë¬¸ì œ í•´ê²°

### CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python train_dense.py --batch-size 64
```

### ëŠë¦° í•™ìŠµ ì†ë„
```bash
# CPUê°€ ë³‘ëª©ì´ë©´ worker ìˆ˜ ì¤„ì´ê¸°
python train_dense.py --num-workers 0
```

### ì˜ì¡´ì„± ëˆ„ë½
```bash
pip install --upgrade -r requirements.txt
```

## ğŸ“š ì°¸ê³ ë¬¸í—Œ

1. **Magnitude Pruning**
   - Han et al., "Learning both Weights and Connections for Efficient Neural Networks" (NIPS 2015)

2. **Structured Pruning**
   - Li et al., "Pruning Filters for Efficient ConvNets" (ICLR 2017)

3. **Lottery Ticket Hypothesis**
   - Frankle & Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" (ICLR 2019)

4. **PyTorch Pruning**
   - https://pytorch.org/tutorials/intermediate/pruning_tutorial.html

5. **ResNet**
   - He et al., "Deep Residual Learning for Image Recognition" (CVPR 2016)
